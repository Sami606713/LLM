{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2737a9d3",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "- In this we can do text preprocessing.\n",
    "    - Removing ireelavent character like `URL` or html content.\n",
    "    - Tokenize the word (word tokenize or sentence tokenize).\n",
    "    - Lemitization or steming.\n",
    "    - Convert text into vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9c4c6",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d14383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c481624",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"\n",
    "Generative pre-trained transformers (GPT) are a type of large language model (LLM)[1][2][3] and a prominent framework for generative artificial intelligence.[4][5] They are artificial neural networks that are used in natural language processing tasks.[6] GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content.[2][3] As of 2023, most LLMs have these characteristics[7] and are sometimes referred to broadly as GPTs.[8]\n",
    "\n",
    "The first GPT was introduced in 2018 by OpenAI.[9] OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series.[10] Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023.[11] Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.[1]\n",
    "\n",
    "The term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI,[12] and seven models created by Cerebras in 2023.[13] Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce's \"EinsteinGPT\" (for CRM)[14] and Bloomberg's \"BloombergGPT\" (for finance).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb98160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGenerative pre-trained transformers (GPT) are a type of large language model (LLM)[1][2][3] and a prominent framework for generative artificial intelligence.[4][5] They are artificial neural networks that are used in natural language processing tasks.[6] GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content.[2][3] As of 2023, most LLMs have these characteristics[7] and are sometimes referred to broadly as GPTs.[8]\\n\\nThe first GPT was introduced in 2018 by OpenAI.[9] OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series.[10] Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023.[11] Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.[1]\\n\\nThe term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI,[12] and seven models created by Cerebras in 2023.[13] Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce\\'s \"EinsteinGPT\" (for CRM)[14] and Bloomberg\\'s \"BloombergGPT\" (for finance).\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44c5c3",
   "metadata": {},
   "source": [
    "# Convert text into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e283d65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngenerative pre-trained transformers (gpt) are a type of large language model (llm)[1][2][3] and a prominent framework for generative artificial intelligence.[4][5] they are artificial neural networks that are used in natural language processing tasks.[6] gpts are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content.[2][3] as of 2023, most llms have these characteristics[7] and are sometimes referred to broadly as gpts.[8]\\n\\nthe first gpt was introduced in 2018 by openai.[9] openai has released very influential gpt foundation models that have been sequentially numbered, to comprise its \"gpt-n\" series.[10] each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. the most recent of these, gpt-4, was released in march 2023.[11] such models have been the basis for their more task-specific gpt systems, including models fine-tuned for instruction following—which in turn power the chatgpt chatbot service.[1]\\n\\nthe term \"gpt\" is also used in the names and descriptions of such models developed by others. for example, other gpt foundation models include a series of models created by eleutherai,[12] and seven models created by cerebras in 2023.[13] also, companies in different industries have developed task-specific gpts in their respective fields, such as salesforce\\'s \"einsteingpt\" (for crm)[14] and bloomberg\\'s \"bloomberggpt\" (for finance).\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph=paragraph.lower()\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13bbab",
   "metadata": {},
   "source": [
    "**Remiving Irlevant char**\n",
    "- `pattern` for remiving unnecessary char `r'[^a-zA-Z]'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9c2c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4eeec617",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_text=re.sub(r'[^a-zA-Z]',\" \",paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5db2e00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' generative pre trained transformers  gpt  are a type of large language model  llm           and a prominent framework for generative artificial intelligence        they are artificial neural networks that are used in natural language processing tasks     gpts are based on the transformer architecture  pre trained on large data sets of unlabelled text  and able to generate novel human like content        as of       most llms have these characteristics    and are sometimes referred to broadly as gpts      the first gpt was introduced in      by openai     openai has released very influential gpt foundation models that have been sequentially numbered  to comprise its  gpt n  series      each of these was significantly more capable than the previous  due to increased size  number of trainable parameters  and training  the most recent of these  gpt    was released in march           such models have been the basis for their more task specific gpt systems  including models fine tuned for instruction following which in turn power the chatgpt chatbot service      the term  gpt  is also used in the names and descriptions of such models developed by others  for example  other gpt foundation models include a series of models created by eleutherai      and seven models created by cerebras in           also  companies in different industries have developed task specific gpts in their respective fields  such as salesforce s  einsteingpt   for crm      and bloomberg s  bloomberggpt   for finance   '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b432e45",
   "metadata": {},
   "source": [
    "# Tokenize the word\n",
    "- tokenize the word\n",
    "- remove stop word\n",
    "- lemitize the word\n",
    "- convert text into vectore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9123c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "780a902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93eccc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed907d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generative',\n",
       " 'pre',\n",
       " 'trained',\n",
       " 'transformers',\n",
       " 'gpt',\n",
       " 'type',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " 'llm',\n",
       " 'prominent',\n",
       " 'framework',\n",
       " 'generative',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'artificial',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'used',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " 'gpts',\n",
       " 'based',\n",
       " 'transformer',\n",
       " 'architecture',\n",
       " 'pre',\n",
       " 'trained',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'unlabelled',\n",
       " 'text',\n",
       " 'able',\n",
       " 'generate',\n",
       " 'novel',\n",
       " 'human',\n",
       " 'like',\n",
       " 'content',\n",
       " 'llms',\n",
       " 'characteristics',\n",
       " 'sometimes',\n",
       " 'referred',\n",
       " 'broadly',\n",
       " 'gpts',\n",
       " 'first',\n",
       " 'gpt',\n",
       " 'introduced',\n",
       " 'openai',\n",
       " 'openai',\n",
       " 'released',\n",
       " 'influential',\n",
       " 'gpt',\n",
       " 'foundation',\n",
       " 'models',\n",
       " 'sequentially',\n",
       " 'numbered',\n",
       " 'comprise',\n",
       " 'gpt',\n",
       " 'n',\n",
       " 'series',\n",
       " 'significantly',\n",
       " 'capable',\n",
       " 'previous',\n",
       " 'due',\n",
       " 'increased',\n",
       " 'size',\n",
       " 'number',\n",
       " 'trainable',\n",
       " 'parameters',\n",
       " 'training',\n",
       " 'recent',\n",
       " 'gpt',\n",
       " 'released',\n",
       " 'march',\n",
       " 'models',\n",
       " 'basis',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'gpt',\n",
       " 'systems',\n",
       " 'including',\n",
       " 'models',\n",
       " 'fine',\n",
       " 'tuned',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'turn',\n",
       " 'power',\n",
       " 'chatgpt',\n",
       " 'chatbot',\n",
       " 'service',\n",
       " 'term',\n",
       " 'gpt',\n",
       " 'also',\n",
       " 'used',\n",
       " 'names',\n",
       " 'descriptions',\n",
       " 'models',\n",
       " 'developed',\n",
       " 'others',\n",
       " 'example',\n",
       " 'gpt',\n",
       " 'foundation',\n",
       " 'models',\n",
       " 'include',\n",
       " 'series',\n",
       " 'models',\n",
       " 'created',\n",
       " 'eleutherai',\n",
       " 'seven',\n",
       " 'models',\n",
       " 'created',\n",
       " 'cerebras',\n",
       " 'also',\n",
       " 'companies',\n",
       " 'different',\n",
       " 'industries',\n",
       " 'developed',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'gpts',\n",
       " 'respective',\n",
       " 'fields',\n",
       " 'salesforce',\n",
       " 'einsteingpt',\n",
       " 'crm',\n",
       " 'bloomberg',\n",
       " 'bloomberggpt',\n",
       " 'finance']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text=[word for word in token if word not in set(stopwords.words('english'))]\n",
    "process_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf69be9",
   "metadata": {},
   "source": [
    "# Lemitize the word\n",
    "- In this we used `Wordnetlemitizer` b/c it is more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6760ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abee608a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generative',\n",
       " 'pre',\n",
       " 'trained',\n",
       " 'transformer',\n",
       " 'gpt',\n",
       " 'type',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " 'llm',\n",
       " 'prominent',\n",
       " 'framework',\n",
       " 'generative',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'artificial',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'used',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'task',\n",
       " 'gpts',\n",
       " 'based',\n",
       " 'transformer',\n",
       " 'architecture',\n",
       " 'pre',\n",
       " 'trained',\n",
       " 'large',\n",
       " 'data',\n",
       " 'set',\n",
       " 'unlabelled',\n",
       " 'text',\n",
       " 'able',\n",
       " 'generate',\n",
       " 'novel',\n",
       " 'human',\n",
       " 'like',\n",
       " 'content',\n",
       " 'llm',\n",
       " 'characteristic',\n",
       " 'sometimes',\n",
       " 'referred',\n",
       " 'broadly',\n",
       " 'gpts',\n",
       " 'first',\n",
       " 'gpt',\n",
       " 'introduced',\n",
       " 'openai',\n",
       " 'openai',\n",
       " 'released',\n",
       " 'influential',\n",
       " 'gpt',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'sequentially',\n",
       " 'numbered',\n",
       " 'comprise',\n",
       " 'gpt',\n",
       " 'n',\n",
       " 'series',\n",
       " 'significantly',\n",
       " 'capable',\n",
       " 'previous',\n",
       " 'due',\n",
       " 'increased',\n",
       " 'size',\n",
       " 'number',\n",
       " 'trainable',\n",
       " 'parameter',\n",
       " 'training',\n",
       " 'recent',\n",
       " 'gpt',\n",
       " 'released',\n",
       " 'march',\n",
       " 'model',\n",
       " 'basis',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'gpt',\n",
       " 'system',\n",
       " 'including',\n",
       " 'model',\n",
       " 'fine',\n",
       " 'tuned',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'turn',\n",
       " 'power',\n",
       " 'chatgpt',\n",
       " 'chatbot',\n",
       " 'service',\n",
       " 'term',\n",
       " 'gpt',\n",
       " 'also',\n",
       " 'used',\n",
       " 'name',\n",
       " 'description',\n",
       " 'model',\n",
       " 'developed',\n",
       " 'others',\n",
       " 'example',\n",
       " 'gpt',\n",
       " 'foundation',\n",
       " 'model',\n",
       " 'include',\n",
       " 'series',\n",
       " 'model',\n",
       " 'created',\n",
       " 'eleutherai',\n",
       " 'seven',\n",
       " 'model',\n",
       " 'created',\n",
       " 'cerebras',\n",
       " 'also',\n",
       " 'company',\n",
       " 'different',\n",
       " 'industry',\n",
       " 'developed',\n",
       " 'task',\n",
       " 'specific',\n",
       " 'gpts',\n",
       " 'respective',\n",
       " 'field',\n",
       " 'salesforce',\n",
       " 'einsteingpt',\n",
       " 'crm',\n",
       " 'bloomberg',\n",
       " 'bloomberggpt',\n",
       " 'finance']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemitize=WordNetLemmatizer()\n",
    "process_text=[lemitize.lemmatize(word) for word in process_text]\n",
    "process_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5aeb1e",
   "metadata": {},
   "source": [
    "# convert text into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20c22255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac409a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9ffd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vector.fit_transform(process_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9674796",
   "metadata": {},
   "source": [
    "# These are the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35b2791d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generative': 35,\n",
       " 'pre': 64,\n",
       " 'trained': 87,\n",
       " 'transformer': 89,\n",
       " 'gpt': 36,\n",
       " 'type': 92,\n",
       " 'large': 48,\n",
       " 'language': 47,\n",
       " 'model': 52,\n",
       " 'llm': 50,\n",
       " 'prominent': 67,\n",
       " 'framework': 33,\n",
       " 'artificial': 3,\n",
       " 'intelligence': 45,\n",
       " 'neural': 56,\n",
       " 'network': 55,\n",
       " 'used': 94,\n",
       " 'natural': 54,\n",
       " 'processing': 66,\n",
       " 'task': 83,\n",
       " 'gpts': 37,\n",
       " 'based': 4,\n",
       " 'architecture': 2,\n",
       " 'data': 19,\n",
       " 'set': 76,\n",
       " 'unlabelled': 93,\n",
       " 'text': 85,\n",
       " 'able': 0,\n",
       " 'generate': 34,\n",
       " 'novel': 57,\n",
       " 'human': 38,\n",
       " 'like': 49,\n",
       " 'content': 16,\n",
       " 'characteristic': 11,\n",
       " 'sometimes': 80,\n",
       " 'referred': 69,\n",
       " 'broadly': 8,\n",
       " 'first': 30,\n",
       " 'introduced': 46,\n",
       " 'openai': 60,\n",
       " 'released': 70,\n",
       " 'influential': 43,\n",
       " 'foundation': 32,\n",
       " 'sequentially': 73,\n",
       " 'numbered': 59,\n",
       " 'comprise': 15,\n",
       " 'series': 74,\n",
       " 'significantly': 78,\n",
       " 'capable': 9,\n",
       " 'previous': 65,\n",
       " 'due': 23,\n",
       " 'increased': 41,\n",
       " 'size': 79,\n",
       " 'number': 58,\n",
       " 'trainable': 86,\n",
       " 'parameter': 62,\n",
       " 'training': 88,\n",
       " 'recent': 68,\n",
       " 'march': 51,\n",
       " 'basis': 5,\n",
       " 'specific': 81,\n",
       " 'system': 82,\n",
       " 'including': 40,\n",
       " 'fine': 29,\n",
       " 'tuned': 90,\n",
       " 'instruction': 44,\n",
       " 'following': 31,\n",
       " 'turn': 91,\n",
       " 'power': 63,\n",
       " 'chatgpt': 13,\n",
       " 'chatbot': 12,\n",
       " 'service': 75,\n",
       " 'term': 84,\n",
       " 'also': 1,\n",
       " 'name': 53,\n",
       " 'description': 20,\n",
       " 'developed': 21,\n",
       " 'others': 61,\n",
       " 'example': 26,\n",
       " 'include': 39,\n",
       " 'created': 17,\n",
       " 'eleutherai': 25,\n",
       " 'seven': 77,\n",
       " 'cerebras': 10,\n",
       " 'company': 14,\n",
       " 'different': 22,\n",
       " 'industry': 42,\n",
       " 'respective': 71,\n",
       " 'field': 27,\n",
       " 'salesforce': 72,\n",
       " 'einsteingpt': 24,\n",
       " 'crm': 18,\n",
       " 'bloomberg': 6,\n",
       " 'bloomberggpt': 7,\n",
       " 'finance': 28}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
